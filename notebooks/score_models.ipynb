{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNZcHDdiTZMK"
   },
   "source": [
    "## Score-based modelling on MNIST\n",
    "\n",
    "In this practical session, we will train a score-based model on MNIST. You will have to implement: \n",
    "\n",
    "1) The forward pass of a U-Net network. \n",
    "\n",
    "Reminder: a U-Net is a multi-scale neural network made of skip connections with concatenation. See there: https://en.wikipedia.org/wiki/U-Net \n",
    "\n",
    "2) The loss function of a score-based network (denoising objective): \n",
    "\\begin{align}\n",
    "  \\mathcal{L}_\\theta &= \\sum^T_{t=1} \\mathbb{E}_{x_0\\sim p_{\\mathrm{data}}, x_t \\sim p_{\\sigma_t}(x_t \\mid x_0)} \\left[ \\left\\| s_{\\theta}(x, t) - \\nabla \\log p_{t} (x_t \\mid x_0) \\right\\|^2_2 \\right] \\\\ &= \\sum^T_{t=1} \\mathbb{E}_{x_0\\sim p_{\\mathrm{data}}, x_t \\sim p_{\\sigma_t}(x_t \\mid x_0)} \\left[ \\left\\| s_{\\theta}(x, t) - \\frac{x_t - x_0}{\\sigma_t^2} \\right\\|^2_2 \\right]\n",
    "\\end{align}\n",
    "\n",
    "3) The annealed Langevin Dynamics sampling algorithm, which runs the Langevin dynamic over the $T$ noise levels (cf. the slides). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6HEsZEnKkjK",
    "outputId": "469f5534-67a4-45c2-a7ec-635140009668"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "kgV0t9SCKurl",
    "outputId": "59698e4f-5d6e-4b72-bff8-3773d3539123"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(-npimg+1, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvyxzi1wLLzz"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch models inherit from torch.nn.Module\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None,n_groups=8):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(n_groups,mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(n_groups,out_channels)\n",
    "        )\n",
    "        self.act = nn.GELU()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                                        nn.GroupNorm(n_groups,out_channels))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.double_conv(x)\n",
    "        x = self.shortcut(x)\n",
    "        x = self.act(out + x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down_conv = nn.Conv2d(in_channels, in_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down_conv(x)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels,sigmas):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        self.inc = DoubleConv(n_channels, 32,n_groups=1)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 128)\n",
    "        #self.mid = DoubleConv(256,256)\n",
    "        self.up1 = Up(256, 64)\n",
    "        self.up2 = Up(128, 32)\n",
    "        self.up3 = Up(64, 32)\n",
    "        self.outc = OutConv(32, n_channels)\n",
    "\n",
    "        self.sigmas = sigmas\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        #x4 = self.mid(x4)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        x = self.outc(x)\n",
    "\n",
    "        used_sigmas = self.sigmas[y].view(x.shape[0], *([1] * len(x.shape[1:])))\n",
    "\n",
    "        x = x / used_sigmas\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_sigmas(sigma_begin,sigma_end,nb_sigma):\n",
    "    sigmas = torch.tensor(\n",
    "        np.exp(np.linspace(np.log(sigma_begin), np.log(sigma_end),\n",
    "                               nb_sigma))).float()\n",
    "    return sigmas\n",
    "\n",
    "def anneal_dsm_score_estimation(scorenet, samples, sigma_index, sigmas, anneal_power=2.):\n",
    "    used_sigmas = sigmas[sigma_index].view(samples.shape[0], *([1] * len(samples.shape[1:])))\n",
    "    \n",
    "    perturbed_samples = samples + torch.randn_like(samples) * used_sigmas\n",
    "    target = - 1 / (used_sigmas ** 2) * (perturbed_samples - samples)\n",
    "    scores = scorenet(perturbed_samples, sigma_index)\n",
    "    target = target.view(target.shape[0], -1)\n",
    "    scores = scores.view(scores.shape[0], -1)\n",
    "    loss = 1 / 2. * ((scores - target) ** 2).sum(dim=-1) * used_sigmas.squeeze() ** anneal_power\n",
    "\n",
    "    return loss.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHmWjVpm08Yi",
    "outputId": "141d6acd-2451-4125-e9eb-d68f30c6725c"
   },
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "  print(\"GPUs available.\")\n",
    "else:\n",
    "  print(\"No GPUs available.\")\n",
    "\n",
    "batch_size = 64\n",
    "sigma_begin = 10\n",
    "sigma_end = 0.01\n",
    "nb_sigma = 500 \n",
    "nb_epochs = 1\n",
    "learning_rate = 0.0005\n",
    "\n",
    "sigma_levels = get_sigmas(sigma_begin,sigma_end,nb_sigma)\n",
    "if cuda_available:\n",
    "  sigma_levels = sigma_levels.cuda(0)\n",
    "print('hierarchy of sigma : ',sigma_levels)\n",
    "model = UNet(1, sigma_levels)\n",
    "if cuda_available:\n",
    "  model = model.cuda(0)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create data loaders for our datasets\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZwSi1gQlmM1"
   },
   "source": [
    "You should reach a loss of approximately 30/35 in 5 epochs! Go to the sampling part only you reached this loss level, otherwise it will not give good generated samples. It would be perfect to reach a loss below 30. If you have access to GPUs, go for more epochs and increase the capacity (feature maps width) of the U-Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBkLKRGoLBRm",
    "outputId": "8993fb18-3e55-41d8-c6cd-eebd9a0aa199"
   },
   "outputs": [],
   "source": [
    "for e in range(nb_epochs):\n",
    "    running_loss = 0. \n",
    "    freq_print = 50\n",
    "    \n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, _ = data\n",
    "        \n",
    "        # Draw a random batch of integers: it will be used to select random noise levels in the anneal_dsm_score_estimation\n",
    "        sigma_index = torch.randint(0,nb_sigma,(inputs.shape[0],))\n",
    "\n",
    "        if cuda_available:\n",
    "          inputs = inputs.cuda(0)\n",
    "          sigma_index = sigma_index.cuda(0)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        loss = anneal_dsm_score_estimation(model,inputs,sigma_index,sigma_levels)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % freq_print == (freq_print-1):\n",
    "            last_loss = running_loss / freq_print # loss per batch\n",
    "            print('epoch {}  batch {} loss: {}'.format(e + 1, i + 1, last_loss))\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1VqdYTlCXp9",
    "outputId": "0db80151-ced1-4ce1-e421-489647a6426b"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sampling(model,sigma_levels,n_per_sigma=5,step_lr=0.0000062,cuda_available=False):\n",
    "    #@torch.no_grad()\n",
    "    #def sampling(model,sigma_levels,n_per_sigma=100,step_lr=0.00002):\n",
    "    x = torch.randn((32,1,32,32)) \n",
    "    if cuda_available:\n",
    "        x = x.cuda(0) \n",
    "    x = x * sigma_levels[0]\n",
    "    for i in range(sigma_levels.shape[0]):\n",
    "        sigma = sigma_levels[i]\n",
    "        sigma_index = torch.ones(x.shape[0], device=x.device) * i\n",
    "        sigma_index = sigma_index.long()\n",
    "        if cuda_available:\n",
    "          sigma_index = sigma_index.cuda(0)\n",
    "        step_size = step_lr * (sigma / sigma_levels[-1]) ** 2\n",
    "        for n in range(n_per_sigma):\n",
    "            noise = torch.randn_like(x)\n",
    "            score_estimation = model(x,sigma_index)\n",
    "            x = x + step_size * score_estimation + torch.sqrt(step_size * 2) * noise\n",
    "    return x\n",
    "\n",
    "x = sampling(model,sigma_levels,cuda_available=cuda_available)\n",
    "if cuda_available:\n",
    "    x = x.detach().cpu()\n",
    "x = torch.clip(x,-1,1)\n",
    "img_grid = torchvision.utils.make_grid(x)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
