{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Beq_KrufOKyb"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnVV6uKMON2O"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlALLaZZOPak",
    "outputId": "7c058d24-dad6-477b-a334-ce55fdbf9f01"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNB1eBJQORLO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten, Reshape, \\\n",
    "  Conv2D, Conv2DTranspose, UpSampling2D, BatchNormalization, \\\n",
    "  LeakyReLU, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wCQGY5NxjLA"
   },
   "source": [
    "We are going to use FashionMNIST for this session. The objective will be to build a generative adversarial network for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5al-XDELOVM8",
    "outputId": "d46f2f4f-c523-477e-bc29-8ec9e8b7a2d9"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "T37iKqo2OYku",
    "outputId": "2f7a954d-e570-42f1-bf48-ee8f027a0316"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(0, 18):\n",
    "    plt.subplot(3, 6, i + 1)\n",
    "    plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30DbdpLMOaad"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ASsbcIfOe-5"
   },
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "seed_dim = 32\n",
    "intermediate_dim = 256\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzGO6oVwVRNZ"
   },
   "outputs": [],
   "source": [
    "def adam_optimizer(lr=0.0001):\n",
    "    return tf.keras.optimizers.Adam(lr=lr, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn2uUzBIw19d"
   },
   "source": [
    "## A) Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9paUGtuUUU1f"
   },
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# * create a simple fully-connected generator that takes noise as input and generate an image\n",
    "# * plot the output of the generator for some input sample\n",
    "# * Hint: we need pixels between -1 and 1. What is the correct activation for the last layer ?\n",
    "\n",
    "def create_generator():\n",
    "    \n",
    "    inp = Input(shape=(seed_dim,))\n",
    "    x = Dense(units=256,input_dim=seed_dim)(inp)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dense(units=512)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dense(units=original_dim, activation='tanh')(x)\n",
    "    out = Reshape((28, 28, 1))(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=inp,\n",
    "        outputs=out,\n",
    "        name=\"generator\"\n",
    "    )\n",
    "    ####\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KD-tKE4pVCnc",
    "outputId": "c654dfd5-989d-4673-a37f-f2ad80fbf319"
   },
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG4QHNf8w19l"
   },
   "outputs": [],
   "source": [
    "def test_generator(generator):\n",
    "    \"\"\"\n",
    "    Test that the generator generates the correct shape\n",
    "    \"\"\"\n",
    "    seed = tf.random.normal(shape=(batch_size, seed_dim),\n",
    "                            mean=0., stddev=1.)\n",
    "    data = generator(seed).numpy()\n",
    "    \n",
    "    if data.shape != (batch_size, 28, 28, 1):\n",
    "        raise RuntimeError(f\"Shape is {data.shape} (expected {(batch_size, 28, 28, 1)})\")\n",
    "    \n",
    "test_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "-acWBNI7w19o",
    "outputId": "1108d93b-4c5f-49e2-dcf9-7eacde81fed3"
   },
   "outputs": [],
   "source": [
    "####\n",
    "def generate_and_plot(generator, inputs=None):\n",
    "    \n",
    "    if inputs is None:\n",
    "        real_images = x_train[:8]\n",
    "    else:\n",
    "        real_images = inputs\n",
    "\n",
    "    real_images = real_images.reshape((-1, 28, 28, 1))\n",
    "    seed = tf.random.normal(shape=(batch_size, seed_dim),\n",
    "                            mean=0., stddev=1.)\n",
    "\n",
    "    data = generator(seed).numpy()\n",
    "    data = np.concatenate(\n",
    "        (data, real_images), axis=0)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for i in range(0, 16):\n",
    "        plt.subplot(4, 8, i + 1)\n",
    "        plt.imshow(data[i, :].reshape(28, 28), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "generate_and_plot(generator)\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kajf33low19s"
   },
   "source": [
    "## B) Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eB9YSPQ6Vavn"
   },
   "outputs": [],
   "source": [
    "# Exercise:\n",
    "# * Create a simple fully-connected discriminator model\n",
    "#   using a funnel 512 -> 256 -> 128\n",
    "# * Train it to check that it can easily make the difference between \n",
    "#   your random generator and the real images\n",
    "\n",
    "def create_discriminator() -> Model:\n",
    "    \n",
    "    inp = Input(shape=(28, 28, 1))\n",
    "    x = Flatten()(inp)\n",
    "    x = Dense(units=512,input_dim=original_dim)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(units=256)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(units=128)(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Dense(units=1)(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=out, name=\"discriminator\")\n",
    "    ####\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZrIDyZcVatH",
    "outputId": "77072d3e-2841-4a60-b740-02aa2f82882d"
   },
   "outputs": [],
   "source": [
    "discriminator = create_discriminator()\n",
    "print(discriminator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pp6-6bo6w19z",
    "outputId": "0e09365f-3b9b-4a2d-ed04-45c70a79f2a6"
   },
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "seed = tf.random.normal(shape=(n_train, seed_dim), mean=0., stddev=1.)\n",
    "fake_data = generator(seed).numpy()\n",
    "discr_train = tf.concat([x_train[:n_train].reshape(-1, 28, 28, 1), fake_data], axis=0)\n",
    "y_train = np.concatenate([np.zeros(n_train), np.ones(n_train)])\n",
    "\n",
    "seed = tf.random.normal(shape=(10000, seed_dim),\n",
    "                            mean=0., stddev=1.)\n",
    "fake_data = generator(seed).numpy()\n",
    "discr_test = tf.concat([x_test.reshape(-1, 28, 28, 1), fake_data], axis=0)\n",
    "y_test = np.concatenate([np.zeros(10000), np.ones(10000)])\n",
    "print(discr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S012IAckw192",
    "outputId": "bf7df303-8ee5-4ce7-aaaa-1fd5d8a5eb9f"
   },
   "outputs": [],
   "source": [
    "discriminator.fit(x=discr_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnbDcg0tw195",
    "outputId": "78a5dd71-9550-4de7-c551-4bf08e3ce3f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_score=discriminator.predict(discr_test), y_true=y_test)\n",
    "print(f\"AUC is {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOoJFCHLw198"
   },
   "source": [
    "## C) GAN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-0TSEmonn4F"
   },
   "outputs": [],
   "source": [
    "# Exercise 4:\n",
    "# * Implement the training of the GAN. What do you observe ?\n",
    "\n",
    "def training(\n",
    "    iter=2,\n",
    "    batch_size=128,\n",
    "    discr_func=create_discriminator,\n",
    "    gen_func=create_generator,\n",
    "    seed_dim=seed_dim,\n",
    "    soft_labels=True,\n",
    "    non_saturated=True\n",
    "):\n",
    "\n",
    "    # Creating GAN\n",
    "    generator= gen_func()\n",
    "    discriminator= discr_func()\n",
    "    \n",
    "    #gan = create_gan(discriminator, generator)\n",
    "\n",
    "    generator_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "    start_time = time.time()\n",
    "    losses_generator = list()\n",
    "    losses_discriminator = list()\n",
    "\n",
    "    for my_iter in range(iter):\n",
    "        #Loading the batch\n",
    "        real_iter = my_iter % (len(x_train)//batch_size)\n",
    "        real_data  = x_train[real_iter*batch_size:(real_iter+1)*batch_size]\n",
    "        real_data = real_data.reshape(-1, 28, 28, 1)\n",
    "\n",
    "        # Construct different batches of  real and fake data \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as d:\n",
    "          d.watch(discriminator.trainable_variables)\n",
    "          #Add code here          \n",
    "          noise = np.random.normal(0, 1, [batch_size, seed_dim])\n",
    "          generated_images = generator(noise)\n",
    "          \n",
    "          d_real = tf.math.sigmoid(discriminator(real_data))\n",
    "          d_fake = tf.math.sigmoid(discriminator(generated_images))\n",
    "          \n",
    "          loss_discriminator = -(tf.reduce_mean(tf.math.log(d_real)) + tf.reduce_mean(tf.math.log(1-d_fake)))\n",
    "          # Backward pass\n",
    "          grads = d.gradient(loss_discriminator, discriminator.trainable_variables)\n",
    "          discriminator_opt.apply_gradients(\n",
    "              zip(grads, discriminator.trainable_variables)\n",
    "          )\n",
    "        losses_discriminator.append(loss_discriminator)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, [batch_size, seed_dim])\n",
    "        with tf.GradientTape() as g:\n",
    "          g.watch(generator.trainable_variables)\n",
    "          # Forward pass\n",
    "          d_fake = tf.math.sigmoid(discriminator(generator(noise)))\n",
    "          loss_generator = tf.reduce_mean(tf.math.log(1-d_fake)) ##Vanilla: -loss_discrim, minimize proba to be classified as fake\n",
    "          grads = g.gradient(loss_generator, generator.trainable_variables)\n",
    "          generator_opt.apply_gradients(\n",
    "              zip(grads, generator.trainable_variables)\n",
    "          )\n",
    "        losses_generator.append(loss_generator.numpy())\n",
    "\n",
    "        if my_iter % 100 == 0:\n",
    "            clear_output()\n",
    "            print(f\"Iter {my_iter} ({real_iter})\")\n",
    "            if start_time is not None:\n",
    "                time_spent = time.time() - start_time\n",
    "                print(f\"(Avg {time_spent/100} seconds per iteration)\")\n",
    "            start_time = time.time()\n",
    "            print(my_iter, loss_discriminator, tf.reduce_mean(d_real), tf.reduce_mean(d_fake))\n",
    "            generate_and_plot(generator, real_data)\n",
    "        \n",
    "    return losses_generator, losses_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYYmZvSmaTc1"
   },
   "outputs": [],
   "source": [
    "#EXERCISES\n",
    "#1) Write code for GANs\n",
    "#2) Add soft labels \n",
    "#3) Write non-saturated GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "eNUAxpM8GB8r",
    "outputId": "a7080da1-af02-40bd-81f1-a5a859b8b2bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_generator, losses_discriminator = training(10000, 64, soft_labels=False, non_saturated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRFfyVGbzVPY"
   },
   "outputs": [],
   "source": [
    "#losses_generator, losses_discriminator = training(5000, 64, soft_labels=True, non_saturated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ewg9So-CAAL"
   },
   "outputs": [],
   "source": [
    "#losses_generator, losses_discriminator = training(10000, 64, soft_labels=False, non_saturated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "o1KXxFmWw1-D",
    "outputId": "b44a8f80-ac26-4d90-a7f3-81f26d066485"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.subplots()\n",
    "ax.plot(pd.Series(losses_generator).rolling(50).mean(), label=\"Generator\")\n",
    "ax.plot(pd.Series(losses_discriminator).rolling(50).mean(), label=\"Discriminator\")\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhVBwV7R6PnU"
   },
   "source": [
    "## D) Switching to CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpblQMiYJGpk",
    "outputId": "edcd9bba-0edd-4cb7-d615-b04b35a240e3"
   },
   "outputs": [],
   "source": [
    "def create_generator_cnn(seed_dim=seed_dim):\n",
    " \n",
    "\n",
    "    inp = Input(shape=(seed_dim,))\n",
    "    x = Dense(7*7*256, use_bias=False)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = Reshape((7, 7, 256))(x)\n",
    "    x = Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "\n",
    "    x = Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh')(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=inp,\n",
    "        outputs=x,\n",
    "        name=\"generator\"\n",
    "    )\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return model\n",
    "\n",
    "create_generator_cnn().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hb-6ZaK66OU",
    "outputId": "0fb06df7-ab6c-4f8a-8002-4f7301d13d6c"
   },
   "outputs": [],
   "source": [
    "def create_discriminator_cnn():\n",
    "    inp = Input(shape=(28, 28, 1))\n",
    "    x = Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[28,28,1])(inp)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv2D(128,(5,5), strides=(2,2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=inp,\n",
    "        outputs=x,\n",
    "        name=\"discriminator\"\n",
    "    )\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n",
    "    return model\n",
    "\n",
    "create_discriminator_cnn().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "nNgmU9R57pGi",
    "outputId": "2a9a9e7f-4684-4c2a-8cb1-613972227220"
   },
   "outputs": [],
   "source": [
    "losses_generator, losses_discriminator = training(1000, 64, create_discriminator_cnn, create_generator_cnn, soft_labels=False, non_saturated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "7r92IO2mW4Cj",
    "outputId": "075dc640-85a4-4100-de31-6b2867dec014"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.subplots()\n",
    "ax.plot(pd.Series(losses_generator).rolling(30).mean(), label=\"Generator\")\n",
    "ax.plot(pd.Series(losses_discriminator).rolling(30).mean(), label=\"Discriminator\")\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjzDrNcw9HBI"
   },
   "source": [
    "## E) OPT. Wasserstein Gan\n",
    "\n",
    "Exercise: Implement a Wasserstein Gan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lexHzY6o2FM"
   },
   "outputs": [],
   "source": [
    "# Exercise 4:\n",
    "# * Implement the training of the GAN. What do you observe ?\n",
    "def gradient_penalty(discriminator, x, x_gen):\n",
    "        epsilon = tf.random.uniform([x.shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "        x_hat = epsilon * x + (1 - epsilon) * x_gen\n",
    "        with tf.GradientTape() as t:\n",
    "            t.watch(x_hat)\n",
    "            d_hat = discriminator(x_hat)\n",
    "        gradients = t.gradient(d_hat, x_hat)\n",
    "        ddx = tf.sqrt(tf.reduce_sum(gradients ** 2, axis=[1, 2]))\n",
    "        d_regularizer = tf.reduce_mean((ddx - 1.0) ** 2)\n",
    "        return d_regularizer\n",
    "\n",
    "def training_wasserstein(\n",
    "    iter=2,\n",
    "    batch_size=128,\n",
    "    discr_func=create_discriminator,\n",
    "    gen_func=create_generator,\n",
    "    seed_dim=seed_dim,\n",
    "    cnn = True\n",
    "):\n",
    "\n",
    "    # Creating GAN\n",
    "    generator= gen_func()\n",
    "    discriminator= discr_func()\n",
    "    gradient_penalty_weight = 10\n",
    "    generator_opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    discriminator_opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    start_time = time.time()\n",
    "    losses_generator = list()\n",
    "    losses_discriminator = list()\n",
    "\n",
    "    for my_iter in range(iter):\n",
    "\n",
    "        #Loading the batch\n",
    "        real_iter = my_iter % (len(x_train)//batch_size)\n",
    "        real_data  = x_train[real_iter*batch_size:(real_iter+1)*batch_size]\n",
    "        real_data = real_data.reshape(-1, 28, 28, 1)\n",
    "\n",
    "        # Train discriminator\n",
    "        noise = np.random.normal(0, 1, [batch_size, seed_dim])\n",
    "        generated_images = generator(noise)\n",
    "        with tf.GradientTape() as g:\n",
    "          g.watch(discriminator.trainable_variables)\n",
    "          \n",
    "          pred_real = discriminator(real_data)\n",
    "          pred_fake = discriminator(generated_images)\n",
    "          \n",
    "          loss_discriminator = (tf.reduce_mean(pred_real) - tf.reduce_mean(pred_fake))\n",
    "          loss_discriminator -= gradient_penalty_weight*gradient_penalty(discriminator, real_data, generated_images)\n",
    "          grads = g.gradient(-loss_discriminator, discriminator.trainable_variables)\n",
    "          discriminator_opt.apply_gradients(\n",
    "              zip(grads, discriminator.trainable_variables)\n",
    "          )\n",
    "        losses_discriminator.append(loss_discriminator.numpy())\n",
    "        \n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, [batch_size, seed_dim])\n",
    "        with tf.GradientTape() as g:\n",
    "          g.watch(generator.trainable_variables)\n",
    "          pred = discriminator(generator(noise))\n",
    "          loss_generator = tf.reduce_mean(pred)\n",
    "          grads = g.gradient(-loss_generator, generator.trainable_variables)\n",
    "          generator_opt.apply_gradients(\n",
    "             zip(grads, generator.trainable_variables)\n",
    "          )\n",
    "        losses_generator.append(loss_generator.numpy())\n",
    "        \n",
    "        if my_iter % 100 == 0:\n",
    "            clear_output()\n",
    "            print(f\"Iter {my_iter} ({real_iter})\")\n",
    "            if start_time is not None:\n",
    "                time_spent = time.time() - start_time\n",
    "                print(f\"(Avg {time_spent/100} seconds per iteration)\")\n",
    "                print(tf.reduce_mean(pred_real), tf.reduce_mean(pred_fake))\n",
    "            start_time = time.time()\n",
    "            generate_and_plot(generator, real_data)\n",
    "        \n",
    "    return losses_generator, losses_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "NrI3ydLtE5FN",
    "outputId": "04b1ad6c-7c4b-460f-da2c-94779e063b23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses_generator_w, losses_discriminator_w = training_wasserstein(1000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "33dPsUyKTPl9",
    "outputId": "b389e042-87d5-417a-f7b6-6e49e446e3ba"
   },
   "outputs": [],
   "source": [
    "losses_generator_cnn, losses_discriminator_cnn = training_wasserstein(3000, 64, discr_func=create_discriminator_cnn, gen_func=create_generator_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPaznTLbE8KH"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.subplots()\n",
    "ax.plot(pd.Series(losses_generator_w).rolling(30).mean(), label=\"Generator\")\n",
    "ax.plot(pd.Series(losses_discriminator_w).rolling(30).mean(), label=\"Discriminator\")\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGmKqNOuTdLr"
   },
   "outputs": [],
   "source": [
    "# Exercise 4:\n",
    "# * Implement the training of the GAN. What do you observe ?\n",
    "def gradient_penalty(discriminator, x, x_gen):\n",
    "        epsilon = tf.random.uniform([x.shape[0], 1, 1, 1], 0.0, 1.0)\n",
    "        x_hat = epsilon * x + (1 - epsilon) * x_gen\n",
    "        with tf.GradientTape() as t:\n",
    "            t.watch(x_hat)\n",
    "            d_hat = discriminator(x_hat)\n",
    "        gradients = t.gradient(d_hat, x_hat)\n",
    "        ddx = tf.sqrt(tf.reduce_sum(gradients ** 2, axis=[1, 2]))\n",
    "        d_regularizer = tf.reduce_mean((ddx - 1.0) ** 2)\n",
    "        return d_regularizer\n",
    "\n",
    "def training_vanilla_GANs_regularized(\n",
    "    iter=2,\n",
    "    batch_size=128,\n",
    "    discr_func=create_discriminator,\n",
    "    gen_func=create_generator,\n",
    "    seed_dim=seed_dim,\n",
    "    cnn = True\n",
    "):\n",
    "\n",
    "    # Creating GAN\n",
    "    generator= gen_func()\n",
    "    discriminator= discr_func()\n",
    "    gradient_penalty_weight = 10\n",
    "    generator_opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    discriminator_opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    start_time = time.time()\n",
    "    losses_generator = list()\n",
    "    losses_discriminator = list()\n",
    "\n",
    "    for my_iter in range(iter):\n",
    "\n",
    "        #Loading the batch\n",
    "        real_iter = my_iter % (len(x_train)//batch_size)\n",
    "        real_data  = x_train[real_iter*batch_size:(real_iter+1)*batch_size]\n",
    "        real_data = real_data.reshape(-1, 28, 28, 1)\n",
    "\n",
    "        # Train discriminator\n",
    "        noise = np.random.normal(0, 1, [batch_size, seed_dim])\n",
    "        generated_images = generator(noise)\n",
    "        with tf.GradientTape() as g:\n",
    "          g.watch(discriminator.trainable_variables)\n",
    "          \n",
    "          pred_real = tf.math.sigmoid(discriminator(real_data))\n",
    "          pred_fake = tf.math.sigmoid(discriminator(generated_images))\n",
    "          \n",
    "          loss_discriminator = (tf.reduce_mean(tf.math.log(pred_real)) + tf.reduce_mean(tf.math.log(1-pred_fake)))\n",
    "          loss_discriminator -= gradient_penalty_weight*gradient_penalty(discriminator, real_data, generated_images)\n",
    "          \n",
    "          grads = g.gradient(-loss_discriminator, discriminator.trainable_variables)\n",
    "          discriminator_opt.apply_gradients(\n",
    "              zip(grads, discriminator.trainable_variables)\n",
    "          )\n",
    "        losses_discriminator.append(loss_discriminator.numpy())\n",
    "        \n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, [batch_size, seed_dim])\n",
    "        with tf.GradientTape() as g:\n",
    "          g.watch(generator.trainable_variables)\n",
    "          pred = tf.math.sigmoid(discriminator(generator(noise)))\n",
    "          loss_generator = tf.reduce_mean(tf.math.log(pred))\n",
    "          grads = g.gradient(-loss_generator, generator.trainable_variables)\n",
    "          generator_opt.apply_gradients(\n",
    "             zip(grads, generator.trainable_variables)\n",
    "          )\n",
    "        losses_generator.append(loss_generator.numpy())\n",
    "        \n",
    "        if my_iter % 100 == 0:\n",
    "            clear_output()\n",
    "            print(f\"Iter {my_iter} ({real_iter})\")\n",
    "            if start_time is not None:\n",
    "                time_spent = time.time() - start_time\n",
    "                print(f\"(Avg {time_spent/100} seconds per iteration)\")\n",
    "                print(tf.reduce_mean(pred_real), tf.reduce_mean(pred_fake))\n",
    "            start_time = time.time()\n",
    "            generate_and_plot(generator, real_data)\n",
    "        \n",
    "    return losses_generator, losses_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "oHqfH2RNTxjj",
    "outputId": "36d05746-bfe3-477a-d67a-9eca15576557"
   },
   "outputs": [],
   "source": [
    "losses_generator_cnn, losses_discriminator_cnn = training_vanilla_GANs_regularized(3500, 64, discr_func=create_discriminator_cnn, gen_func=create_generator_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "Y0tmNmdBWcO9",
    "outputId": "65917524-716e-4354-98d4-ef1fb3fa7828"
   },
   "outputs": [],
   "source": [
    "losses_generator_cnn, losses_discriminator_cnn = training(1500, 64, discr_func=create_discriminator_cnn, gen_func=create_generator_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4CIHfxIWpJd"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.subplots()\n",
    "ax.plot(pd.Series(losses_generator_w).rolling(30).mean(), label=\"Generator\")\n",
    "ax.plot(pd.Series(losses_discriminator_w).rolling(30).mean(), label=\"Discriminator\")\n",
    "plt.legend()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
